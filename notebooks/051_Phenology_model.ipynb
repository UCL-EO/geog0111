{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img alt='UCL' src=\"images/ucl_logo.png\" align='center'>\n",
    "\n",
    "\n",
    "[<img src=\"images/noun_post_2109127.svg\" width=\"50\" align='right'>](070_Summary.ipynb)\n",
    "[<img src=\"images/noun_pre_2109128.svg\" width=\"50\" align='right'>](050_Models.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 051 A Phenology model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the Nothern Hemisphere, and for temperate latitudes, there is a clear seasonal cycle in vegetation, particularly visible in leaf area index (LAI). We have seen that we have access to global LAI data over the last 20-odd years from MODIS. One of the indicators of a changing climate is a change in the pattern of vegetation dynamics. A warmer climate typically means that vegetation appears earlier in the year. \n",
    "\n",
    "We call a model that describes the phases of plant development over the year a phenology model. There are many types we could consider, some of the more useful of which are driven by environmental data such as temperature. In that way, we can try to explain how much of the variation we see can be explained by inter-annual temperature variations. A simpler 'descriptive' model that is often used is the \"double logistic\" curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Mathematically, the function predicts the e.g. LAI (or some vegetation index) as\n",
    "\n",
    "$$\n",
    "y = p_0 - p_1\\cdot\\left[\\frac{1}{1+\\exp\\left(p_2\\cdot(t-p_3)\\right)} + \\frac{1}{1+\\exp\\left(-p_4\\cdot(t-p_5)\\right)} - 1\\right].\n",
    "$$\n",
    "\n",
    "If we inspect this form, we can see that $p_0$ and $p_1$ scale the vertical span of the function, whereas $p_3$ and $p_5$ are some sort of temporal shift, and the remaining parameters $p_2$ and $p_4$ control the slope of the two flanks. \n",
    "\n",
    "![double sigmoid](data/doublesig.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's access some LAI data that we have used before, and also look at the regularised data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try again ... or enter 'exit' as username to quit\n",
      "--> user login required for ['https://e4ftl01.cr.usgs.gov'] <--\n",
      "Enter your username: dr.p.e.lewis@gmail.com\n",
      "please type your password········\n",
      "please re-type your password for confirmation········\n",
      "password created\n",
      "error in stitchModisDate: {}\n",
      "{'product': 'MCD15A3H', 'tile': ['h17v03', 'h18v03', 'h17v04', 'h18v04'], 'year': 2019, 'doys': [1], 'sds': ['Lai_500m']}\n",
      "Problem with doy 1 ... continuing ...\n"
     ]
    }
   ],
   "source": [
    "from geog0111.modisUtils import getLai\n",
    "from geog0111.modisUtils import regularise\n",
    "import numpy as np\n",
    "\n",
    "# load some data\n",
    "tile    = ['h17v03','h18v03','h17v04','h18v04']\n",
    "year    = 2019\n",
    "fips    = \"LU\"\n",
    "    \n",
    "lai,std,doy =  getLai(year,tile,fips)\n",
    "std[std<1] = 1\n",
    "weight = np.zeros_like(std)\n",
    "mask = (std > 0)\n",
    "weight[mask] = 1./(std[mask]**2)\n",
    "weight[lai > 10] = 0.\n",
    "\n",
    "interpolated_lai = regularise(lai,weight,5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p0,p1 = (107,72)\n",
    "x_size,y_size=(10,5)\n",
    "\n",
    "fig, axs = plt.subplots(1,1,figsize=(x_size,y_size))\n",
    "x = doy\n",
    "axs.plot(x,lai[:,p0,p1],'+',label='LAI')\n",
    "axs.plot(x,interpolated_lai[:,p0,p1],'g',label='smoothed LAI')\n",
    "axs.plot(x,weight[:,p0,p1],'+',label='weight')\n",
    "\n",
    "axs.set_title(f'{p0} {p1}')\n",
    "# ensure the same scale for all\n",
    "axs.set_ylim(0,7)\n",
    "axs.set_xlabel('DOY 2019')\n",
    "axs.set_ylabel('LAI')\n",
    "axs.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a function for the model, provide an initial estimate at the model parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dbl_sigmoid_function(p, t):\n",
    "    \"\"\"The double sigmoid function defined over t (where t is an array).\n",
    "    Takes a vector of 6 parameters\"\"\"\n",
    "\n",
    "    sigma1 = 1./(1+np.exp(p[2]*(t-p[3])))\n",
    "    sigma2 = 1./(1+np.exp(-p[4]*(t-p[5])))\n",
    "    y = p[0] - p[1]*(sigma1 + sigma2 - 1)\n",
    "    return y\n",
    "\n",
    "t = np.arange(1, 366)\n",
    "p = np.array([0.5, 4, 0.07, 100, 0.07, 260])\n",
    "y = dbl_sigmoid_function(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and plot the observations and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p0,p1 = (107,72)\n",
    "\n",
    "fig, axs = plt.subplots(1,1,figsize=(x_size,y_size))\n",
    "x = doy\n",
    "axs.plot(x,lai[:,p0,p1],'+',label='LAI')\n",
    "axs.plot(x,interpolated_lai[:,p0,p1],'g',label='smoothed LAI')\n",
    "axs.plot(t,y,'r',label='model')\n",
    "axs.plot(x,weight[:,p0,p1],'+',label='weight')\n",
    "\n",
    "axs.set_title(f'{p0} {p1}')\n",
    "# ensure the same scale for all\n",
    "axs.set_ylim(0,7)\n",
    "axs.set_xlabel('DOY 2019')\n",
    "axs.set_ylabel('LAI')\n",
    "axs.legend(loc='best')\n",
    "print('Luxembourg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even with a rough guess of the model parameters, we can produce a plot that is strikingly similar to the regularised result we saw in previous sessions. That result shows what might be interpreted as a double feature around the maximum LAI, but we have seen that spurious features of this sort can arise from the filtering process. We might consider the logistic model a better interpretation of the data then. The double-logistic model has the additional advantage that the parameters have a physical meaning that we can map from year to year and compare. The regularisation result is itself of course, the result of a model. The main difference then is that the regularisation result is from a non-parametric model, and the double-logistic  is a parametric model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that there are 6 model parameters. In the previous session, we learned rapid methods of parameter estimation that could only work with low dimensional data. We need to consider how to simply this model then, if we are to apply LUT approaches.\n",
    "\n",
    "One idea is to attempt to normalise the vertical axis of the model. If we suppose we know the maximum and minimum LAI, we can use these as estimates of $p_0$ and $p_1$. One route to this would be to use the regularised model output. Next, we might assume that the slope parameters $p_2$ and $p_4$ are equal, without too much loss of generalisation. As a further simplification, we might assume we know this parameter, and fix it, at a value of 0.07 say. We can later look at the impact of these assumptions, but they have allowed us to reduce the problem to a 2-D one that we can easily solve with a LUT.\n",
    "\n",
    "The remaining parameters $p_3$ and $p_5$ the times at which the curve most rapidly increases or decreases. For a LUT, these can be better phrased as:\n",
    "\n",
    "           width  = p5 - p3\n",
    "           centre = (p3 + p5)/2\n",
    "           \n",
    "so:\n",
    "\n",
    "           p5 =  centre + width/2\n",
    "           p3 =  centre - width/2\n",
    "\n",
    "We might bound the width at between 50 and 250 days, and the centre at by 100 to 300 days. We must take the definition of time as modulo 365 to allow the model to operate in the Southern Hemisphere.\n",
    "\n",
    "![double sigmoid centre/width](data/doublesig2.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simplified model and dataset then is built assuming $p_0$ and $p_1$ are given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T19:27:36.186520Z",
     "start_time": "2018-11-14T19:27:35.702036Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "param0 = np.min(interpolated_lai,axis=0)\n",
    "param1 = np.max(interpolated_lai,axis=0) - param0\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(14,8))\n",
    "axs=axs.flatten()\n",
    "im = axs[0].imshow(param1,interpolation=\"nearest\",\\\n",
    "                vmax=7,cmap=plt.cm.inferno_r)\n",
    "axs[0].set_title('p1')\n",
    "fig.colorbar(im, ax=axs[0])\n",
    "im = axs[1].imshow(param0,interpolation=\"nearest\",\\\n",
    "                vmax=7,cmap=plt.cm.inferno_r)\n",
    "axs[1].set_title('p0')\n",
    "fig.colorbar(im, ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Define an interface to the simplified function\n",
    "which we feed with known values in p\n",
    "but ones we will wish to solve for in sp\n",
    "'''\n",
    "def simple_dbl_sigmoid_function(sp,p, t):\n",
    "    \"\"\"The double sigmoid function defined over t (where t is an array).\n",
    "    p is vector of 6 parameters\n",
    "    sp is 2 parameters: width and centre\"\"\"\n",
    "    width,centre = sp\n",
    "    p[5] =  (centre + width/2)\n",
    "    p[3] =  (centre - width/2)\n",
    "    return dbl_sigmoid_function(p,t)\n",
    "\n",
    "# use np.newaxis to resolve shapes of t and parameters\n",
    "t = np.arange(1, 366)[:,np.newaxis,np.newaxis]\n",
    "sp = [200.,200.]\n",
    "\n",
    "# set up parameter list\n",
    "p = [param0[np.newaxis,:,:],param1[np.newaxis,:,:],0.07,sp[0],0.07,sp[1]]\n",
    "\n",
    "# calculate y\n",
    "y = simple_dbl_sigmoid_function(sp,p,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p0,p1 = (107,72)\n",
    "\n",
    "fig, axs = plt.subplots(1,1,figsize=(x_size,y_size))\n",
    "x = doy\n",
    "axs.plot(x,lai[:,p0,p1],'+',label='LAI')\n",
    "axs.plot(x,interpolated_lai[:,p0,p1],'g',label='smoothed LAI')\n",
    "axs.plot(t[:,0,0],y[:,p0,p1],'r',label='model')\n",
    "axs.plot(x,weight[:,p0,p1],'+',label='weight')\n",
    "\n",
    "axs.set_title(f'{p0} {p1}')\n",
    "# ensure the same scale for all\n",
    "axs.set_ylim(0,7)\n",
    "axs.set_xlabel('DOY 2019')\n",
    "axs.set_ylabel('LAI')\n",
    "axs.legend(loc='best')\n",
    "print('Luxembourg: Simplified model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed previously, since we are doing pixel-by-pixel processing, we can change the 2D spatial data into a 1D flattened array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param0_ = param0.ravel()\n",
    "param1_ = param1.ravel()\n",
    "print(f'param1  shape:  {param1.shape}')\n",
    "print(f'param1_ shape:  {param1_.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'lai shape:    {lai.shape}')\n",
    "print(f'weight shape: {weight.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same for the observations and weight but using `reshape` following the method in [032_More_numpy](032_More_numpy.ipynb#Simplifying-shape:-flatten,-ravel,-reshape-and-unravel_index):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newshape = (lai.shape[0],lai[0].size)\n",
    "print(newshape)\n",
    "# reshape \n",
    "lai_    = lai.reshape(newshape)\n",
    "weight_ = weight.reshape(newshape)\n",
    "\n",
    "print(f'shape of lai  : {lai.shape}')\n",
    "print(f'shape of lai_ : {lai_.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the previous exercise, we need to reconcile the observations and modelling time grid. In this case all we need do is to use `doy` in place of `t`.\n",
    "\n",
    "We will form axes:\n",
    "\n",
    "    (time,image index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = [200.,200.]\n",
    "\n",
    "t = doy[:,np.newaxis]\n",
    "p = [param0_[np.newaxis,:],param1_[np.newaxis,:],0.07,sp[0],0.07,sp[1]]\n",
    "print(f't    shape:  {t.shape}')\n",
    "print(f'p[1] shape:  {p[1].shape}')\n",
    "\n",
    "y = simple_dbl_sigmoid_function(sp,p,t)\n",
    "print(f'y shape:  {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted error\n",
    "err = (lai_ - y) * weight_\n",
    "# mean over time\n",
    "rmse = np.sqrt(np.mean(err*err,axis=0))\n",
    "print(rmse.shape)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,1,figsize=(5,5))\n",
    "im = axs.imshow(rmse.reshape(lai[0].shape),interpolation=\"nearest\",\\\n",
    "                cmap=plt.cm.inferno_r)\n",
    "axs.set_title(f'RMSE for parameters {sp}')\n",
    "fig.colorbar(im, ax=axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a 2-D LUT as previously, and provide a flattened version. We have defined bounds above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mgrid as previously to define a 2D grid of parameters\n",
    "sp0min,sp0max,sp0step = 100,250,10\n",
    "sp1min,sp1max,sp1step = 100,300,10\n",
    "sp0,sp1 = np.mgrid[sp0min:sp0max+sp0step:sp0step,\\\n",
    "                 sp1min:sp1max+sp1step:sp1step]\n",
    "sp0_ = sp0.ravel()\n",
    "sp1_ = sp1.ravel()\n",
    "\n",
    "print(f'shape of sp0  : {sp0.shape}')\n",
    "print(f'shape of sp0_ : {sp0_.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-process the LAI just to make sure we have it handy and re-shaped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newshape = (lai.shape[0],lai[0].size)\n",
    "print(newshape)\n",
    "# reshape \n",
    "lai_    = lai.reshape(newshape)\n",
    "weight_ = weight.reshape(newshape)\n",
    "\n",
    "print(f'shape of lai  : {lai.shape}')\n",
    "print(f'shape of lai_ : {lai_.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning the dimensions now is a little more complex, but we can revert to using the original model. We have one dimension that is time, one space, and one parameter space using the flattened arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions: [t,npix,nsamp]\n",
    "# more complex for this case\n",
    "sp0_ext    = sp0_[np.newaxis,np.newaxis,:]\n",
    "sp1_ext    = sp1_[np.newaxis,np.newaxis,:]\n",
    "lai_ext    = lai_[:,:,np.newaxis]\n",
    "weight_ext = weight_[:,:,np.newaxis]\n",
    "t_ext      = doy[:,np.newaxis,np.newaxis]\n",
    "param0_ext = param0_[np.newaxis,:,np.newaxis]\n",
    "param1_ext = param1_[np.newaxis,:,np.newaxis]\n",
    "param2_ext = 0.07\n",
    "param3_ext = sp1_ext - sp0_ext/2.\n",
    "param4_ext = 0.07\n",
    "param5_ext = sp1_ext + sp0_ext/2.\n",
    "\n",
    "# print out to check they line up\n",
    "print(f'sp0_ext:    {sp0_ext.shape}')\n",
    "print(f'sp1_ext:    {sp1_ext.shape}')\n",
    "print(f'param0_ext: {param0_ext.shape}')\n",
    "print(f'param1_ext: {param1_ext.shape}')\n",
    "print(f'lai_ext:    {lai_ext.shape}')\n",
    "print(f'weight_ext: {weight_ext.shape}')\n",
    "print(f't_ext:      {t_ext.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = [sp0_ext,sp1_ext]\n",
    "p = [param0_ext,param1_ext,param2_ext,\\\n",
    "     param3_ext,param4_ext,param5_ext]\n",
    "\n",
    "y = dbl_sigmoid_function(p,t_ext)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_ext = (y - lai_ext)*weight_ext\n",
    "error_ext = error_ext*error_ext\n",
    "rmse = np.sqrt(np.mean(error_ext,axis=0))\n",
    "print(rmse.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a value of RMSE for each element on the parameter grid (axis 1), for each pixel in the dataset (axis 0). \n",
    "\n",
    "We next want to know the minimum over the parameter set (`axis 1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minrmse = rmse.min(axis=1)\n",
    "print(minrmse.shape)\n",
    "\n",
    "fig, axs = plt.subplots(1,1,figsize=(5,5))\n",
    "# reshape for plotting\n",
    "im = axs.imshow(minrmse.reshape(lai[0].shape),interpolation=\"nearest\",\\\n",
    "                cmap=plt.cm.inferno_r)\n",
    "axs.set_title(f'Min RMSE')\n",
    "fig.colorbar(im, ax=axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the parameters at the minimum value, simply follow the same approach as previously using `argmin`, but with one extra dimension. In this case, `rmse` has shape `(21350, 336)` so we apply the argmin over the parameter dimension (axis 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use np.argmin to find the minimum over axis 1\n",
    "imin = np.argmin(rmse,axis=1)\n",
    "p0min,p1min = sp0_[imin],sp1_[imin]\n",
    "print(f'1D index shape of minimum  : {imin.shape}')\n",
    "print(f'parameter shape at minimum : {p0min.shape},{p1min.shape}')\n",
    "\n",
    "width, centre = p0min,p1min\n",
    "p3 = centre - width/2\n",
    "p5 = centre + width/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reshape p3,p5 before plotting\n",
    "\n",
    "fig, axs = plt.subplots(2,2,figsize=(10,10))\n",
    "axs = axs.flatten()\n",
    "ishape = lai[0].shape\n",
    "\n",
    "im = axs[0].imshow(param0.reshape(ishape),interpolation=\"nearest\",\\\n",
    "                cmap=plt.cm.inferno_r)\n",
    "axs[0].set_title(f'p0')\n",
    "fig.colorbar(im, ax=axs[0])\n",
    "\n",
    "im = axs[1].imshow(param1.reshape(ishape),interpolation=\"nearest\",\\\n",
    "                cmap=plt.cm.inferno_r)\n",
    "axs[1].set_title(f'p1')\n",
    "fig.colorbar(im, ax=axs[1])\n",
    "\n",
    "im = axs[2].imshow(p3.reshape(ishape),interpolation=\"nearest\",\\\n",
    "                cmap=plt.cm.inferno_r)\n",
    "axs[2].set_title(f'p3')\n",
    "fig.colorbar(im, ax=axs[2])\n",
    "\n",
    "im = axs[3].imshow(p5.reshape(ishape),interpolation=\"nearest\",\\\n",
    "                cmap=plt.cm.inferno_r)\n",
    "axs[3].set_title(f'p5')\n",
    "fig.colorbar(im, ax=axs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
    "axs = axs.flatten()\n",
    "\n",
    "im = axs[0].imshow(width.reshape(ishape),interpolation=\"nearest\",\\\n",
    "                cmap=plt.cm.inferno_r)\n",
    "axs[0].set_title(f'width')\n",
    "fig.colorbar(im, ax=axs[0])\n",
    "\n",
    "im = axs[1].imshow(centre.reshape(ishape),interpolation=\"nearest\",\\\n",
    "                cmap=plt.cm.inferno_r)\n",
    "axs[1].set_title(f'centre')\n",
    "fig.colorbar(im, ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now forward model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param3_ext = p3[np.newaxis,:,np.newaxis]\n",
    "param5_ext = p5[np.newaxis,:,np.newaxis]\n",
    "\n",
    "p = [param0_ext,param1_ext,param2_ext,\\\n",
    "     param3_ext,param4_ext,param5_ext]\n",
    "\n",
    "# reshape to original lai shape\n",
    "y = dbl_sigmoid_function(p,t_ext).reshape(lai.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualise our test pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise RMSE for this pixel\n",
    "# shape is npixels,ngrid\n",
    "# RMSE full grid is nx,ny, npx,npy\n",
    "newshape = (*(lai.shape[1:]),*(sp0.shape))\n",
    "print(f'rmse full shape: {newshape}')\n",
    "rmse_grid = rmse.reshape(newshape)[p0,p1]\n",
    "min_rmse = rmse_grid.min()\n",
    "\n",
    "# use np.argmin to find the minimum\n",
    "imin = np.argmin(rmse_grid)\n",
    "ip0min,ip1min = np.unravel_index(imin,sp0.shape)\n",
    "\n",
    "p0min,p1min = sp0.ravel()[imin],sp1.ravel()[imin]\n",
    "print(f'1D index of minimum  : {imin}')\n",
    "print(f'parameter at minimum : {p0min},{p1min}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p0,p1 = (107,72)\n",
    "\n",
    "fig, axs = plt.subplots(1,1,figsize=(x_size,y_size))\n",
    "x = doy\n",
    "axs.plot(x,lai[:,p0,p1],'+',label='LAI')\n",
    "axs.plot(x,interpolated_lai[:,p0,p1],'g',label='smoothed LAI')\n",
    "axs.plot(doy,y[:,p0,p1],'r',label='model')\n",
    "axs.plot(x,weight[:,p0,p1],'+',label='weight')\n",
    "\n",
    "axs.set_title(f'{p0} {p1}: parameters {p0min} {p1min}')\n",
    "# ensure the same scale for all\n",
    "axs.set_ylim(0,7)\n",
    "axs.set_xlabel('DOY 2019')\n",
    "axs.set_ylabel('LAI')\n",
    "axs.legend(loc='best')\n",
    "print('Luxembourg optimised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the reshaped RMSE values returned from this as an image\n",
    "\n",
    "# plot it\n",
    "fig, axs = plt.subplots(1,1,figsize=(15,8))\n",
    "im = axs.imshow(rmse_grid,interpolation=\"nearest\",\\\n",
    "                vmax=5*min_rmse,cmap=plt.cm.inferno_r)\n",
    "fig.colorbar(im, ax=axs)\n",
    "axs.set_title(f'{p0} {p1}: parameters {p0min} {p1min}')\n",
    "axs.set_xlabel('p0')\n",
    "axs.set_ylabel('p1')\n",
    "plt.plot([ip1min],[ip0min],'r+',label=\"minimum RMSE\")\n",
    "axs.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the criteria we discussed in [050_Models](050_Models.ipynb#Visualisation-of-RMSE), we can see this as quite a good stable solution: it is not close to the boundaries, and we can see the shape of the erro functiuon quite well. The error function is perhaps a little flat, but it has a good form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Exercise 1\n",
    "\n",
    "* From the code above, develop a function `solver` in a file `work/lut_solver.py` that takes the following inputs:\n",
    "    \n",
    "    * lai, weight : datasets of shape (Nt,Nx,Ny) for observations and reliability\n",
    "    * 2D parameter grids for model parameters p3 and p5 with shape (Np0,Np1)\n",
    "    * function slope parameters p2 and p4 for the double sigmoid: float\n",
    "    * function vertical min and extent parameters p0 and p1 of shape (Nx,Ny)\n",
    "    \n",
    "  and solves for the optimal weighted fit between LAI and modelled LAI using the parameters pi\n",
    "  It should return:\n",
    "  \n",
    "    *  p : list of 6 parameter arrays solved for, so 6 of shape (Nx,Ny)\n",
    "    * RMSE : the RMSE \n",
    "* Produce a plot of all 6 model parameters    \n",
    "* The code could be made more efficient by not processing invalid pixels. Develop and use a mask of valid pixels to implement this.\n",
    "\n",
    "Hint: You might usefully define some utility functions such as `get_lai` and `get_p0p1` to allow you to easily load the datasets you need top run. You might base these around `geog0111.get_lai_data` and `geog0111.regularise`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from geog0111.modisUtils import getLai\n",
    "from geog0111.modisUtils import get_weight\n",
    "from geog0111.modisUtils import regularise\n",
    "from geog0111.lut_solver import lut_solver,get_lai,get_p0p1\n",
    "import numpy as np\n",
    "\n",
    "lai,weight,doy = getLai()\n",
    "sp0min,sp0max,sp0step = 100,250,10\n",
    "sp1min,sp1max,sp1step = 100,300,10\n",
    "sp0,sp1 = np.mgrid[sp0min:sp0max+sp0step:sp0step,\\\n",
    "                 sp1min:sp1max+sp1step:sp1step]\n",
    "width,centre = sp0,sp1\n",
    "sp3 = centre - width/2.\n",
    "sp5 = centre + width/2.\n",
    "p2 = p4 = 0.07\n",
    "p0,p1 = get_p0p1(lai,weight)\n",
    "\n",
    "rmse,p = lut_solver(doy,lai,weight,p0,p1,p2,sp3,p4,sp5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# The code could be made more efficient \n",
    "# by not processing invalid pixels. \n",
    "# Develop and use a mask of valid pixels to implement this.\n",
    "msg = '''Just apply a mask to the ravelled image data'''\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# * Produce a plot of all 6 model parameters   \n",
    "\n",
    "fig, axs = plt.subplots(2,3,figsize=(14,10))\n",
    "axs = axs.flatten()\n",
    "for i in range(axs.shape[0]):\n",
    "    im = axs[i].imshow(p[i],interpolation=\"nearest\",\\\n",
    "                    cmap=plt.cm.inferno_r)\n",
    "    axs[i].set_title(f'p{i}')\n",
    "    fig.colorbar(im, ax=axs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have shown that, with an initial estimate of 4 of the 6 model parameters, we can use a quite sparse 2-D LUT to provide a mapping of phenolology parameters for a given year for a country the size of Luxembourg in very fast time. The approach could be refined, for instance by other pair-wise inversions to refine the solution, but the improvement in this would likely be quite small. This is evidenced by the low overall RMSE for most of the country (all pixels RMSE less than 1.2 in weighted LAI).\n",
    "\n",
    "For geospatial model parameter estimation, rapid methods such as this are vital because of the very large dataset dimensions. \n",
    "\n",
    "To achieve this LUT-based parameter estimation, we have had to make extensive use of the fast array-processing facilities of `numpy`. As with the previous section, this involved building parameter grids using `np.mgrid`, and careful matching of dataset dimensions with `np.newaxis`. We used `np.argmin` to find the LUT index with the minimum RMSE value, for each pixel in the scene. This was slightly complicated by needing to flatten the LUT dimensions use a 1-D index for the minimum RMSE, and involved use of `np.reshape` and `np.flatten`. \n",
    "\n",
    "The programming task here is quite complex, but a very practical one. You will not need to anything quite this complex for formal coursework submission (i.e. to pass the course) but you should all be able to follow and understand the approach taken here, and re-implement it for any similar situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "[<img src=\"images/noun_post_2109127.svg\" width=\"50\" align='right'>](070_Summary.ipynb)\n",
    "[<img src=\"images/noun_pre_2109128.svg\" width=\"50\" align='right'>](050_Models.ipynb)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda env:geog0111-geog0111",
   "language": "python",
   "name": "conda-env-geog0111-geog0111-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
