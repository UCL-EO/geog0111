{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt='UCL' src=\"images/ucl_logo.png\" align='center'>\n",
    "\n",
    "\n",
    "[<img src=\"images/noun_post_2109127.svg\" width=\"50\" align='right'>](021_Streams.ipynb)\n",
    "[<img src=\"images/noun_pre_2109128.svg\" width=\"50\" align='right'>](018_Running_Python.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 020 Files and other Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "\n",
    "### Purpose\n",
    "\n",
    "In this session, we will learn about files and similar resources. We will introduce the standard Python library [`pathlib`](https://docs.python.org/3/library/pathlib.html) which is how we deal with file paths, as well as the local package [gurlpath](geog0111/gurlpath.py) that allows a similar object-oriented approach with files and other objects on the web. The treatment is intentionally high-level. If you need to go deeper into calls to URLs, you should look at the Python packages: [urllib.parse](https://docs.python.org/3.7/library/urllib.parse.html), [requests](https://requests.readthedocs.io/en/master/) and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "You will need some understanding of the following:\n",
    "\n",
    "\n",
    "* [001 Using Notebooks](001_Notebook_use.ipynb)\n",
    "* [002 Unix](002_Unix.ipynb) with a good familiarity with the UNIX commands we have been through.\n",
    "* [003 Getting help](003_Help.ipynb)\n",
    "* [004_Accounts](004_Accounts.ipynb)\n",
    "* [005_Packages](005_Packages.ipynb)\n",
    "* [010 Variables, comments and print()](010_Python_Introduction.ipynb)\n",
    "* [011 Data types](011_Python_data_types.ipynb) \n",
    "* [012 String formatting](012_Python_strings.ipynb)\n",
    "* [013_Python_string_methods](013_Python_string_methods.ipynb)\n",
    "\n",
    "### Test\n",
    "\n",
    "You should run a [NASA account test](004_Accounts.ipynb) if you have not already done so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data resources\n",
    "\n",
    "### Resource location\n",
    "\n",
    "We store information on a computer in files, or file-like resources. We will use the term 'file' below to mean either of these concepts, other than specific issues relating to particular types of file/resource.\n",
    "\n",
    "To get information from files, we need to be able to specify some **address** for the file/resource location, along with some way of interacting with the file. These concepts are captured in the idea of a [URI](https://en.wikipedia.org/wiki/Uniform_Resource_Identifier) (Uniform Resource Indicator). You will most likely have come across the related idea of a [Uniform Resource Locator (URL)](https://en.wikipedia.org/wiki/URL), which is a URL such as [https://www.geog.ucl.ac.uk/people/academic-staff/philip-lewis](https://www.geog.ucl.ac.uk/people/academic-staff/philip-lewis)\n",
    "that gives:\n",
    "\n",
    "* the location of the resource: `people/academic-staff/philip-lewis`\n",
    "* the access and interpretation protocol: [`https`](https://en.wikipedia.org/wiki/HTTPS) (secure [`http`](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol))\n",
    "* the network domain name: [`www.geog.ucl.ac.uk`](https://www.geog.ucl.ac.uk)\n",
    "\n",
    "When we visit this URL using an appropriate tool such as a browser, the tool can access and interpret the information in the resource: in this case, interpret the [html code](https://www.w3schools.com/html) in the file pointed to by the URL.\n",
    "\n",
    "Similarly, we will be used to the idea of accessing `files` on the computer. These may be in the local file system, or on some network or cloud storage that might be accessible from the local file system. An example of such a file would be some Python code file such as \n",
    "[`geog0111/helloWorld.py`](http://localhost:8888/edit/notebooks/geog0111/helloWorld.py).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binary and text data\n",
    "\n",
    "It is useful at this point to distinguish text (ASCII) and binary resources. Text resources are in a human-readable format, so you can just directly 'look at' the file contents to see what is there. Examples of this are [csv-format](https://en.wikipedia.org/wiki/Comma-separated_values) files, [html pages](https://en.wikipedia.org/wiki/HTML), [yaml](https://en.wikipedia.org/wiki/YAML) or [json](https://en.wikipedia.org/wiki/JSON) configuration files, and even these [Jupyter notebooks](https://jupyter.org/). \n",
    "\n",
    "Information in text files is, as noted, easily readable: you can open the file in any text editor to see the contents. However, for large datasets, and datasets with particular structures (e.g. on grids), it is very inefficient.\n",
    "\n",
    "Typical **binary** files include (most) image data and compressed data (e.g. [zip](https://en.wikipedia.org/wiki/Zip_(file_format)) files), where size is more important, as well as encrypted data, where human-readability might not be desirable.\n",
    "\n",
    "Text files designed for different purposes will mostly have some format definition or conventions. This means that although we can easily scan e.g. a [json file](https://json.org/example.html`), there is a set way of laying such files out, so we can interpret them by eye or in software.\n",
    "\n",
    "Binary files will also have some defined format, but in this case, the user is generally forced to use some software reader to interpret the data correctly.\n",
    "\n",
    "When you wish to access some file or dataset, you will generally know which format the file will be in, so can target an appropriate reader. Quite often though, we have to we might need to examine a dataset before deciding on some parameters for an interpreter, particularly with text datasets. \n",
    "\n",
    "In these notes, we will learn how to access and use binary and text datasets in Python, but from the local file system, and online from URLs.\n",
    "\n",
    "We first need to learn some core tools for file and URL access. We will be using the object-oriented [`pathlib`](https://docs.python.org/3/library/pathlib.html) package for local files, and a local variant of [`urlpath`](https://github.com/chrono-meter/urlpath) called [`gurlpath`](geog0111/gurlpath.py).\n",
    "\n",
    "We will be also using `yaml`, `json` and [`pandas`](https://pandas.pydata.org/) packages for reading particular text file types, and introducing [`gdal`](https://gdal.org/python/) for binary image files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Path`, `import`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have come across the idea of packages in [005 Packages](005_Packages.ipynb). `Path` is part of the `pathlib` package, so in any Python codes, we first must import this into our workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This imports the module `Path` from the library `pathlib`.\n",
    "\n",
    "We may repeat this `import` in the codes below, for illustration purposes. You need only import it once in any file or session though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `posix`\n",
    "\n",
    "The main class we will use from `pathlib` is `Path`. This provides an object-oriented way of dealing with files and paths, that is portable to any operating system. Recall from [002 Unix](002_Unix.ipynb) that unix-like operating systems use `posix` filenames, separated by forward slash `/` (or just *slash*). Windows uses a backslash `\\`. But *we* want to write Python codes that are generic and shouldn't need to greatly worry about such issues. Using `Path` greatly helps in this regard, though there will always the occasional time we do need to distinguish `posix` and non-`posix` systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Common `Path` methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with a table of [commonly-used methods](https://stackabuse.com/introduction-to-the-python-pathlib-module/#:~:text=The%20Pathlib%20module%20can%20deal,(usually%20the%20current%20directory).) using `Path` and give their [Unix equivalents](002_Unix.ipynb), most of which we have already learned.\n",
    "\n",
    "|command| unix-equivalent | purpose|\n",
    "|---|---|---|\n",
    "|`Path.cwd()`| `pwd` |Return path object representing the current working directory|\n",
    "|`Path.home()`| `~`| Return path object representing the home directory|\n",
    "|`Path.stat()`| `ls -l`* | return info about the path. File size is `Path.stat().st_size` |\n",
    "|`Path.chmod()`| `chmod` | change file mode and permissions|\n",
    "|`Path.glob(pattern)`| `ls *` | Glob the pattern given in the directory that is represented by the path, yielding matching files of any kind|\n",
    "|`Path.mkdir()`| `mkdir` | to create a new directory at the given path|\n",
    "|`Path.rename()`| `mv` | Rename a file or directory to the given target|\n",
    "|`Path.rmdir()`| `rmdir` | Remove the empty directory|\n",
    "|`Path.unlink()`| `rm` | Remove the file or symbolic link|\n",
    "\n",
    "`*` Really, `Path.stat()` equates to the `unix` command `stat`, but this contains the information we access using `ls -l`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are already familiar with most of these commands in `unix`, we can get straight into using them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am in directory /nfs/cfs/home3/Uucfa6/ucfalew/geog0111/notebooks\n",
      "My home is /home/ucfalew\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(f'I am in directory {Path.cwd()}')\n",
    "print(f'My home is {Path.home()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the filenames generic, we can form a filename from a list using `Path()`, so `Path('bin','README')` would refer to the filename `bin/README` on a `posix` system, and `bin/README` on Windows. However, this is interpreted the same as just using `bin/README` which will often be clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they are the same\n"
     ]
    }
   ],
   "source": [
    "assert Path('bin','README') == Path('bin/README')\n",
    "print('they are the same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function `exists` to check if a file exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does bin/README exist? True\n"
     ]
    }
   ],
   "source": [
    "readme_file = Path('bin/README')\n",
    "print(f'Does {readme_file} exist? {readme_file.exists()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a sub-directory or file to a `Path` object, we can use the list form, or we can just specify the separator '/' between the components, provided the first term is a `Path` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting from the relative directory \"bin\": bin\n",
      "README file is: bin/README\n",
      "README file is: bin/README\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# the relative directory bin\n",
    "bindir = Path('bin')\n",
    "print(f'starting from the relative directory \"bin\": {bindir}')\n",
    "\n",
    "# add README to the path\n",
    "readme_file = Path(bindir,'README')\n",
    "print(f'README file is: {readme_file}')\n",
    "\n",
    "# another way to do it using /\n",
    "readme_file = bindir/'README'\n",
    "print(f'README file is: {readme_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Exercise 1\n",
    "\n",
    "There is a file called `environment.yml` in the directory `copy`.\n",
    "\n",
    "* use `Path` to generate the a variable `copy_dir` containing the pathname of the `copy` directory\n",
    "* create a variable `env_file` which adds add the file `environment.yml` to this \n",
    "* check to see if the file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "does copy/environment.yml exist? True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# ANSWER\n",
    "\n",
    "# There is a file called environment.yml in the directory copy.\n",
    "# use Path to generate the a variable copy_dir containing the \n",
    "# pathname of the copy directory\n",
    "copy_dir = Path('copy')\n",
    "\n",
    "# create a variable env_file which adds add the file \n",
    "# environment.yml to this\n",
    "env_file = copy_dir / 'environment.yml'\n",
    "# or\n",
    "env_file = Path(copy_dir,'environment.yml')\n",
    "\n",
    "# check to see if the file exists\n",
    "print(f'does {env_file} exist? {env_file.exists()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a directory, use:\n",
    "\n",
    "    Path().mkdir()\n",
    "    \n",
    "We can use the options:\n",
    "\n",
    "    Path().mkdir(parents=True,exist_ok=True)\n",
    "    \n",
    "which will make all of the sub-directories needed, and also no fail if the directory already exists. This is often the safest way to make a directory in your code, provided you are sure that your path will be reasonable.\n",
    "\n",
    "To remove an empty directory, use:\n",
    "\n",
    "    Path().rmdir()\n",
    "    \n",
    "Note that there is no recursive delete in `pathlib`: you have to delete files and directories explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files in tmp: []\n",
      "files in tmp: [PosixPath('tmp/inside')]\n"
     ]
    }
   ],
   "source": [
    "# make a temp directory\n",
    "tmp = Path('tmp')\n",
    "tmp.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "# look inside tmp\n",
    "print(f'files in {tmp}: {list(tmp.glob(\"*\"))}')\n",
    "\n",
    "# generate directory inside\n",
    "inside = tmp / 'inside'\n",
    "# create this\n",
    "inside.mkdir(parents=True,exist_ok=True)\n",
    "# look inside tmp\n",
    "print(f'files in {tmp}: {list(tmp.glob(\"*\"))}')\n",
    "\n",
    "# delete inside if it is a directory\n",
    "inside.is_dir and inside.rmdir()\n",
    "# then delete tmp\n",
    "tmp.is_dir and tmp.rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File information\n",
    "\n",
    "The file permissions format we are used to from `ls -l` is accessed through `filename.stat().st_mode` but needs to be converted to octal to match to `ls`\n",
    "\n",
    "    oct(filename.stat().st_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README file is bin/README\n",
      "       size is 16 bytes\n",
      "       mode is 0o100644\n",
      "      owner is ucfalew\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# similar information to ls -l\n",
    "readme=Path('bin','README')\n",
    "print(f'README file is {readme}')\n",
    "print(f'       size is {readme.stat().st_size} bytes')\n",
    "print(f'       mode is {oct(readme.stat().st_mode)}')\n",
    "print(f'      owner is {readme.owner()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `glob` generators \n",
    "\n",
    "To use a wildcard (or any pattern) to refer to a list of files, we use `Path.glob()` (or `Path.rglob()` for a recursive list). This function returns a generator, which is a special type of list whereby the items in the list are produced on demand (as we use them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use glob to get a list of filenames in the directory bin \n",
    "# that end with .sh -> pattern *.sh using a wildcard\n",
    "filenames = Path('bin').glob('*.sh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator does not have all of the attributes of a list, so we cannot, for example, know the length, withjout first converting it tot a list. It is intended that you step through it one item at a time. This is usually done in a `for` loop, or similar construct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 0 is bin/git-remove-all.sh\n",
      "file 1 is bin/link-set.sh\n",
      "file 2 is bin/notebook-mkdocs.sh\n",
      "file 3 is bin/notebook-run.sh\n",
      "file 4 is bin/setup.sh\n",
      "file 5 is bin/shellMe.sh\n",
      "file 6 is bin/set-course.sh\n",
      "file 7 is bin/init.sh\n",
      "file 8 is bin/howmany.sh\n",
      "file 9 is bin/sort-db.sh\n",
      "file 10 is bin/database.sh\n",
      "file 11 is bin/init0111.sh\n",
      "file 12 is bin/tidy.sh\n",
      "file 13 is bin/get-datasets.sh\n",
      "file 14 is bin/clean0111.sh\n",
      "file 15 is bin/fixA.sh\n"
     ]
    }
   ],
   "source": [
    "for i,f in enumerate(filenames):\n",
    "    print(f'file {i} is {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of a generator is that it will generally need less memory than fully calculating all items in a list. Once we move on to the next item in the generator, any memory used by current item is freed.\n",
    "\n",
    "But is the size of objects is small, you will not notice much impact if you convert the generator to a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('bin/git-remove-all.sh'), PosixPath('bin/link-set.sh'), PosixPath('bin/notebook-mkdocs.sh'), PosixPath('bin/notebook-run.sh'), PosixPath('bin/setup.sh'), PosixPath('bin/shellMe.sh'), PosixPath('bin/set-course.sh'), PosixPath('bin/init.sh'), PosixPath('bin/howmany.sh'), PosixPath('bin/sort-db.sh'), PosixPath('bin/database.sh'), PosixPath('bin/init0111.sh'), PosixPath('bin/tidy.sh'), PosixPath('bin/get-datasets.sh'), PosixPath('bin/clean0111.sh'), PosixPath('bin/fixA.sh')]\n"
     ]
    }
   ],
   "source": [
    "filenames = list(Path('bin').glob('*.sh'))\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `glob` now to get the file permissions of each file `n*` in the directory `bin`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin/notebook-mkdocs.sh   \n",
      "bin/notebook-run.sh      \n"
     ]
    }
   ],
   "source": [
    "# use glob to get a list of filenames in the directory bin \n",
    "# that end with .sh -> pattern n* using a wildcard\n",
    "filenames = Path('bin').glob('n*')\n",
    "for f in filenames:\n",
    "    print(f'{str(f):25s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "* Use `Path` to show the file permissions of all files that end `.sh` in the directory `bin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin/notebook-mkdocs.sh    : 0o100755\n",
      "bin/notebook-run.sh       : 0o100755\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "# use glob to get a list of filenames in the directory bin \n",
    "# that end with .sh -> pattern n* using a wildcard\n",
    "filenames = Path('bin').glob('n*')\n",
    "\n",
    "# loop over the filenames and print the permissions\n",
    "# as octal. Note how we use :25s to line items up\n",
    "for f in filenames:\n",
    "    print(f'{str(f):25s} : {oct(f.stat().st_mode)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin/git-remove-all.sh\n",
      "bin/link-set.sh\n",
      "bin/notebook-mkdocs.sh\n",
      "bin/notebook-run.sh\n",
      "bin/setup.sh\n",
      "bin/shellMe.sh\n",
      "bin/set-course.sh\n",
      "bin/init.sh\n",
      "bin/howmany.sh\n",
      "bin/sort-db.sh\n",
      "bin/database.sh\n",
      "bin/init0111.sh\n",
      "bin/tidy.sh\n",
      "bin/get-datasets.sh\n",
      "bin/clean0111.sh\n",
      "bin/fixA.sh\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "# Use Path to show the file permissions of\n",
    "# all files that end .sh in the directory bin\n",
    "\n",
    "# use glob to get a list of filenames in the directory bin \n",
    "# that end with .sh -> pattern *.sh using a wildcard\n",
    "filenames = Path('bin').glob('*.sh')\n",
    "# loop over the filenames\n",
    "for f in filenames:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `absolute`, `parts`, `name`, `parent`\n",
    "\n",
    "We can use `Path` to convert filenames between relative and absolute representations using `absolute()` and `relative_to()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am in /nfs/cfs/home3/Uucfa6/ucfalew/geog0111/notebooks\n",
      "original relative name:\n",
      "\tbin/README\n",
      "absolute name:\n",
      "\t/nfs/cfs/home3/Uucfa6/ucfalew/geog0111/notebooks/bin/README\n",
      "name relative to /nfs/cfs/home3/Uucfa6/ucfalew/geog0111/notebooks:\n",
      "\tbin/README\n"
     ]
    }
   ],
   "source": [
    "print(f'I am in {Path.cwd()}')\n",
    "\n",
    "# define a relative path name\n",
    "readme=Path('bin/README')\n",
    "print(f'original relative name:\\n\\t{readme}')\n",
    "\n",
    "# convert to absolute\n",
    "readme = readme.absolute()\n",
    "print(f'absolute name:\\n\\t{readme}')\n",
    "\n",
    "# now make a relative pathname, \n",
    "# reletive to current working directory\n",
    "readme = readme.relative_to(Path.cwd())\n",
    "print(f'name relative to {Path.cwd()}:\\n\\t{readme}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite often we need to split some filename up into its constituent parts. This is achieved with `paths` which gives a list of the file name tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bin', 'README')\n"
     ]
    }
   ],
   "source": [
    "readme=Path('bin/README')\n",
    "print(readme.parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use that to get at the filename `README` from the last item in the list, but a more object-oriented way is to use `name` (and `parent` to get the directory up to that point):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name   of bin/README is README\n",
      "parent of bin/README is bin\n"
     ]
    }
   ],
   "source": [
    "readme=Path('bin','README')\n",
    "print(f'name   of {readme} is {readme.name}')\n",
    "print(f'parent of {readme} is {readme.parent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Exercise 3\n",
    "\n",
    "* print out the absolute pathname of the directory that `images/ucl.png` is in\n",
    "* check that the file exists\n",
    "* if it does, print the size of the file in KB to two decimal places\n",
    "\n",
    "You will need to know how many Bytes in a Kilobyte, and how to [format a string to two decimal places](012_Python_strings.ipynb#String-formating). You will also need to remember how to use [`if` statements](015_Python_control.ipynb#Comparison-Operators-and-if)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory ucl.png is in is: /nfs/cfs/home3/Uucfa6/ucfalew/geog0111/notebooks/images\n",
      "file size 1956 Bytes ->  1.91 KB\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "\n",
    "# print out the absolute pathname of the \n",
    "# directory that images/ucl.png is in\n",
    "ucl = Path('images','ucl.png')\n",
    "\n",
    "# use absolute and parent\n",
    "# Use name to show how that is helpful\n",
    "print(f'The directory {ucl.name} is in is: {ucl.absolute().parent}')\n",
    "\n",
    "# check that the file exists\n",
    "# if it does ...\n",
    "if ucl.exists():\n",
    "    # print the size of the file in KB to two decimal places\n",
    "\n",
    "    # from above, use stat().st_size\n",
    "    size_in_bytes = ucl.stat().st_size\n",
    "    # 1024 Bytes -> 1 KB\n",
    "    size_in_KB = size_in_bytes/1024\n",
    "    # 2 dp -> : .2f\n",
    "    print(f'file size {size_in_bytes} Bytes -> {size_in_KB : .2f} KB')\n",
    "else:\n",
    "    print(f'file does not exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources from a URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `gurlpath`, `st_size`\n",
    "\n",
    "The library [`gurlpath`](geog0111/gurlpath.py) in [geog0111](geog0111) is designed to operate in a similar manner to `pathlib` for reading data from URLs. It is derived from [`urlpath`](https://github.com/chrono-meter/urlpath)  which in turn is based on [urllib.parse](https://docs.python.org/3/library/urllib.parse.html) and [requests](https://requests.readthedocs.io/en/master/). It uses [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/) for parsing `html` data. At some point you may wish to learn how to use these lower-level packages, but for learning on this course, you will find it convenient to use this higher-level package.\n",
    "\n",
    "The object in `gurlpath` corresponding to `Path` is `URL`.\n",
    "\n",
    "A text file example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote file https://www.metoffice.gov.uk/hadobs/hadukp/data/monthly/HadSEEP_monthly_qc.txt\n",
      "size is 4.77 KB\n"
     ]
    }
   ],
   "source": [
    "from geog0111.gurlpath import URL\n",
    "site = 'https://www.metoffice.gov.uk/'\n",
    "site_dir = 'hadobs/hadukp/data/monthly'\n",
    "site_file = 'HadSEEP_monthly_qc.txt'\n",
    "\n",
    "url = URL(site,site_dir,site_file)\n",
    "print(f'remote file {url}')\n",
    "print(f'size is {url.stat().st_size/1024 :.2f} KB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A binary file example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote file https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.01.01/MCD15A3H.A2020001.h08v06.006.2020006032951.hdf\n",
      "size is -0.00 KB\n"
     ]
    }
   ],
   "source": [
    "from geog0111.gurlpath import URL\n",
    "\n",
    "site = 'https://e4ftl01.cr.usgs.gov'\n",
    "site_dir = 'MOTA/MCD15A3H.006/2020.01.01'\n",
    "site_file = 'MCD15A3H.A2020001.h08v06.006.2020006032951.hdf'\n",
    "\n",
    "url = URL(site,site_dir,site_file)\n",
    "print(f'remote file {url}')\n",
    "print(f'size is {url.stat().st_size/1024 :.2f} KB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have similar functionality in `URL` to `Path` for manipulating filenames, but more limited file information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL    : https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.01.01/MCD15A3H.A2020001.h08v06.006.2020006032951.hdf\n",
      "name   : MCD15A3H.A2020001.h08v06.006.2020006032951.hdf\n",
      "parent : https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.01.01\n"
     ]
    }
   ],
   "source": [
    "print(f'URL    : {url}')\n",
    "print(f'name   : {url.name}')\n",
    "print(f'parent : {url.parent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but also some other helpful ones on the URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor   : https://e4ftl01.cr.usgs.gov\n",
      "hostname : e4ftl01.cr.usgs.gov\n",
      "scheme   : https\n"
     ]
    }
   ],
   "source": [
    "print(f'anchor   : {url.anchor}')\n",
    "print(f'hostname : {url.hostinfo}')\n",
    "print(f'scheme   : {url.scheme}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Exercise 4\n",
    "\n",
    "* create a `URL` object for the file `table.html` in the directory `psd/enso/mei/` on the site `http://www.esrl.noaa.gov/`.\n",
    "* print out the url and check it is `table.html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.esrl.noaa.gov/psd/enso/mei/table.html\n",
      "passed\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "\n",
    "# create a URL object for the file table.html \n",
    "# in the directory psd/enso/mei/ on the site \n",
    "# http://www.esrl.noaa.gov/.\n",
    "\n",
    "site = 'http://www.esrl.noaa.gov/'\n",
    "site_dir = 'psd/enso/mei'\n",
    "site_file = 'table.html'\n",
    "url = URL(site,site_dir,site_file)\n",
    "\n",
    "# print out the url and check it is table.html\n",
    "print(url)\n",
    "assert url.name == site_file\n",
    "print('passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For accessing URLs, will mostly make use of the following functions in `URL` that you will see are very similar to those for `Path`:\n",
    "\n",
    "\n",
    "|function| purpose|\n",
    "|---|---|\n",
    "|`URL.name`|  filename |\n",
    "|`URL.parent`|  parent |\n",
    "|`URL.parts`|  parts |\n",
    "|`URL.stat()`| return info about the file. N.B. Only `URL.stat().st_size` is used for remote files|\n",
    "|`URL.glob(pattern)`| Glob the pattern given in the URL directory, yielding matching files of any kind| \n",
    "|`URL.exists()`|  test to see if a url is accessible |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### login and password\n",
    "\n",
    "Some web resources require you to use a login and password. This can be specified for the `URL` class by with the functiuon `URL.with_userinfo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function with_userinfo in module urlpath:\n",
      "\n",
      "with_userinfo(self, username, password)\n",
      "    Return a new url with the userinfo changed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(URL.with_userinfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main way you can pass your username and password to the relevant functions. However, in any public information (like these notebooks) we do not want to expose sensitive information such as usernames and passwords.\n",
    "\n",
    "To that end `URL` can make use of stored passwords and usernames using the local [cylog](geog0111/cylog.py) package that was covered in [004_Accounts](004_Accounts.ipynb). You should have already tested that your NASA Earthdata login works for files on the site `https://e4ftl01.cr.usgs.gov`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exists`\n",
    "\n",
    "We can use the function `URL.exists()` to test to see if some URL is accessible to us. The function first tries without a password, but if that fails, tries to find an appropriate password and username in the `cyclog` database.\n",
    "\n",
    "Several of the URL functions have `verbose` options (unlike those from `Path`). This is to allow the user to gain more insight into what is going on in accessing the URL. We will use `exists()` with `verbose=True` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> trying https://e4ftl01.cr.usgs.gov/MOLA/MYD11_L2.006/2002.07.04/MYD11_L2.A2002185.0325.006.2015142192613.hdf\n",
      "--> trying get\n",
      "--> trying https://e4ftl01.cr.usgs.gov/MOLA/MYD11_L2.006/2002.07.04/MYD11_L2.A2002185.0325.006.2015142192613.hdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot access https://e4ftl01.cr.usgs.gov/MOLA/MYD11_L2.006/2002.07.04/MYD11_L2.A2002185.0325.006.2015142192613.hdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> failed to connect\n"
     ]
    }
   ],
   "source": [
    "from geog0111.gurlpath import URL\n",
    "from pathlib import Path\n",
    "\n",
    "site = 'https://e4ftl01.cr.usgs.gov'\n",
    "site_dir = '/MOLA/MYD11_L2.006/2002.07.04'\n",
    "site_file = 'MYD11_L2.A2002185.0325.006.2015142192613.hdf'\n",
    "\n",
    "url = URL(site,site_dir,site_file,verbose=True)\n",
    "if url.exists():\n",
    "    print(f'I can access {url}')\n",
    "else:\n",
    "    print(f'I cannot access {url}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we try to access a datafile on the site [`https://e4ftl01.cr.usgs.gov`](https://e4ftl01.cr.usgs.gov) we need to use our NASA Earthdata login and password. This is done automatically withing the call to `url.exists()` or for any function requiring passwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `glob` and NASA datasets\n",
    "\n",
    "The `glob` function is particularly useful for finding datasets on websites, as we might not always know the full file specification. Using `rglob()` (recursive glob) on a website is generally too slow to be worthwhile, so we will avoid using it.\n",
    "\n",
    "Suppose we want to access some NASA hdf files for the product `MCD15A3H` for the data tile `h08v06`. On the site `https://e4ftl01.cr.usgs.gov` the data will be in the folder `'MOTA/MCD15A3H.006`. Data for a particular date then are in a directory of the form `/{year}.{month}.{day}/` where `month` and `day` must be a 2-digit string, left zero-padded (e.g. `02` instead of `2`). Files in there will have the form `*.h08v06*.hdf`. This is a perfect case for the use of `glob`:\n",
    "\n",
    "If we specify a date (1-based for month, so `1` is January):\n",
    "\n",
    "    year, month, day = '2020', '06', '01'\n",
    "    \n",
    "then the directory will be given by:\n",
    "\n",
    "    site_dir = f'MOTA/MCD15A3H.006/{year:4d}.{month:02d}.{day:02d}'\n",
    "    \n",
    "and the file, with wildcard `*` by:\n",
    "\n",
    "    site_file = '*.h08v06*.hdf'\n",
    "\n",
    "we can use `glob` to discover the full URL specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> keeping existing file /nfsshare/groups/jrole001/geog0111/work/e4ftl01.cr.usgs.gov.store\n",
      "--> parsing URLs from html file 1 items\n",
      "--> discovered 1 files with pattern MOTA in https://e4ftl01.cr.usgs.gov/\n",
      "--> keeping existing file /nfsshare/groups/jrole001/geog0111/work/e4ftl01.cr.usgs.gov/MOTA.store\n",
      "--> parsing URLs from html file 1 items\n",
      "--> discovered 1 files with pattern MCD15A3H.006 in https://e4ftl01.cr.usgs.gov/MOTA\n",
      "--> keeping existing file /nfsshare/groups/jrole001/geog0111/work/e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006.store\n",
      "--> parsing URLs from html file 1 items\n",
      "--> discovered 1 files with pattern 2020.06.01 in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006\n",
      "--> keeping existing file /nfsshare/groups/jrole001/geog0111/work/e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.06.01.store\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.06.01/MCD15A3H.A2020153.h08v06.006.2020160231732.hdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> parsing URLs from html file 1 items\n",
      "--> discovered 1 files with pattern *.h08v06*.hdf in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.06.01\n"
     ]
    }
   ],
   "source": [
    "from geog0111.gurlpath import URL\n",
    "\n",
    "# settings\n",
    "product = 'MCD15A3H'\n",
    "year, month, day = '2020', '06', '01'\n",
    "tile = 'h08v06'\n",
    "\n",
    "# url with wildcards\n",
    "site = 'https://e4ftl01.cr.usgs.gov'\n",
    "site_dir = f'MOTA/{product}.006/{year}.{month}.{day}'\n",
    "site_file = f'*.{tile}*.hdf'\n",
    "\n",
    "# get the information\n",
    "url = URL(site,site_dir,verbose=True)\\\n",
    "# convert generator to list to make it easier to understand\n",
    "hdf_urls = url.glob(site_file)\n",
    "for u in hdf_urls:\n",
    "    print(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is extremely useful for dataset discovery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Exercise 5\n",
    "\n",
    "based on the code from above:\n",
    "\n",
    "    # settings\n",
    "    product = 'MCD15A3H'\n",
    "    year, month, day = '2020', '06', '01'\n",
    "    tile = 'h08v06'\n",
    "\n",
    "    # url with wildcards\n",
    "    site = 'https://e4ftl01.cr.usgs.gov'\n",
    "    site_dir = f'MOTA/{product}.006/{year}.{month}.{day}'\n",
    "    site_file = f'*.{tile}*.hdf'\n",
    "\n",
    "    # get the information\n",
    "    url = URL(site,site_dir,verbose=True)\n",
    "    hdf_urls = list(url.glob(site_file))[0]\n",
    "    \n",
    " * write a function called `modis_dataset` with arguments corresponding to the settings above\n",
    " * the function should return the URL objects of the NASA datasets specified by your arguments\n",
    " * your function should be fully documented and include some error checks\n",
    " * run a test of your function, and print the file size in MB for the file pointed to in the URL to 2 decimal places\n",
    " * what happens if you use a wildcard for the date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from geog0111.gurlpath import URL\n",
    "\n",
    "#Â ANSWER 1\n",
    "\n",
    "# write a function called `modis_dataset` \n",
    "# with arguments corresponding to the settings above\n",
    "#\n",
    "# the function should return the URL objects of \n",
    "# the NASA datasets specified by your arguments\n",
    "#\n",
    "def modis_dataset(product, tile, year, month, day,\n",
    "                  verbose=False,\n",
    "                  site='https://e4ftl01.cr.usgs.gov'):\n",
    "    '''\n",
    "    Get URL object list for NASA MODIS products\n",
    "    for the specified product, tile, year, month, day\n",
    "    \n",
    "    Positional Arguments:\n",
    "     \n",
    "    product : str e.g. 'MCD15A3H'\n",
    "    tile    : str e.g. 'h08v06'\n",
    "    year    : str valid 2000-present\n",
    "    month   : str 01-12\n",
    "    day     : str 01-(28,29,30,31)\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    \n",
    "    site     =  'https://e4ftl01.cr.usgs.gov'\n",
    "    verbose  =  False\n",
    "    '''\n",
    "    #Â you should put some tests in\n",
    "    site_dir = f'MOTA/{product}.006/{year}.{month}.{day}'\n",
    "\n",
    "    site_file = f'*.{tile}*.hdf'\n",
    "\n",
    "    url = URL(site,site_dir,verbose=verbose)\n",
    "    hdf_urls = url.glob(site_file)\n",
    "    return hdf_urls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: 1 MB = 1024 * 1024 Bytes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> keeping existing file /nfsshare/groups/jrole001/geog0111/work/e4ftl01.cr.usgs.gov.store\n",
      "--> parsing URLs from html file 1 items\n",
      "--> discovered 1 files with pattern MOTA in https://e4ftl01.cr.usgs.gov/\n",
      "--> keeping existing file /nfsshare/groups/jrole001/geog0111/work/e4ftl01.cr.usgs.gov/MOTA.store\n",
      "--> parsing URLs from html file 1 items\n",
      "--> discovered 1 files with pattern MCD15A3H.006 in https://e4ftl01.cr.usgs.gov/MOTA\n",
      "--> keeping existing file /nfsshare/groups/jrole001/geog0111/work/e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006.store\n",
      "--> parsing URLs from html file 1 items\n",
      "--> discovered 1 files with pattern 2020.06.01 in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006\n",
      "--> keeping existing file /nfsshare/groups/jrole001/geog0111/work/e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.06.01.store\n",
      "--> parsing URLs from html file 1 items\n",
      "--> discovered 1 files with pattern *.h08v06*.hdf in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.06.01\n",
      "--> trying https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.06.01/MCD15A3H.A2020153.h08v06.006.2020160231732.hdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCD15A3H.A2020153.h08v06.006.2020160231732.hdf : -0.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> failed to connect\n"
     ]
    }
   ],
   "source": [
    "# ANSWER 2\n",
    "# run a test of your function, \n",
    "# and print the file size in MB \n",
    "# for the file pointed to in the URL\n",
    "# to 2 decimal places\n",
    "\n",
    "msg = '''\n",
    "Note: 1 MB = 1024 * 1024 Bytes\n",
    "'''\n",
    "print(msg)\n",
    "\n",
    "args = ['MCD15A3H','h08v06','2020','06', '01']\n",
    "hdf_urls = modis_dataset(*args,verbose=True)\n",
    "# test if exist\n",
    "for u in hdf_urls:\n",
    "    print(f'{u.name} : {u.stat().st_size/(1024*1024): .2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: 1 MB = 1024 * 1024 Bytes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> keeping existing file /nfsshare/groups/jrole001/geog0111/work/e4ftl01.cr.usgs.gov.store\n",
      "--> parsing URLs from html file 1 items\n",
      "--> discovered 1 files with pattern MOTA in https://e4ftl01.cr.usgs.gov/\n",
      "--> keeping existing file /nfsshare/groups/jrole001/geog0111/work/e4ftl01.cr.usgs.gov/MOTA.store\n",
      "--> parsing URLs from html file 1 items\n",
      "--> discovered 1 files with pattern MCD15A3H.006 in https://e4ftl01.cr.usgs.gov/MOTA\n",
      "--> keeping existing file /nfsshare/groups/jrole001/geog0111/work/e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006.store\n",
      "--> parsing URLs from html file 4 items\n",
      "--> discovered 4 files with pattern 2020.*.01 in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006\n",
      "--> keeping existing file /nfsshare/groups/jrole001/geog0111/work/e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.01.01.store\n",
      "--> parsing URLs from html file 1 items\n",
      "--> discovered 1 files with pattern *.h08v06*.hdf in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.01.01\n",
      "--> keeping existing file /nfsshare/groups/jrole001/geog0111/work/e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.03.01.store\n",
      "--> parsing URLs from html file 1 items\n",
      "--> discovered 1 files with pattern *.h08v06*.hdf in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.03.01\n",
      "--> keeping existing file /nfsshare/groups/jrole001/geog0111/work/e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.06.01.store\n",
      "--> parsing URLs from html file 1 items\n",
      "--> discovered 1 files with pattern *.h08v06*.hdf in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.06.01\n",
      "--> keeping existing file /nfsshare/groups/jrole001/geog0111/work/e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.09.01.store\n",
      "--> parsing URLs from html file 1 items\n",
      "--> discovered 1 files with pattern *.h08v06*.hdf in https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.09.01\n",
      "--> trying https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.01.01/MCD15A3H.A2020001.h08v06.006.2020006032951.hdf\n",
      "--> failed to connect\n",
      "--> trying https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.03.01/MCD15A3H.A2020061.h08v06.006.2020066032716.hdf\n",
      "--> failed to connect\n",
      "--> trying https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.06.01/MCD15A3H.A2020153.h08v06.006.2020160231732.hdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCD15A3H.A2020001.h08v06.006.2020006032951.hdf : -0.00 MB\n",
      "MCD15A3H.A2020061.h08v06.006.2020066032716.hdf : -0.00 MB\n",
      "MCD15A3H.A2020153.h08v06.006.2020160231732.hdf : -0.00 MB\n",
      "MCD15A3H.A2020245.h08v06.006.2020253152835.hdf : -0.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> failed to connect\n",
      "--> trying https://e4ftl01.cr.usgs.gov/MOTA/MCD15A3H.006/2020.09.01/MCD15A3H.A2020245.h08v06.006.2020253152835.hdf\n",
      "--> failed to connect\n"
     ]
    }
   ],
   "source": [
    "# ANSWER 3\n",
    "# what happens if you use a wildcard for the date?\n",
    "msg = '''\n",
    "Note: 1 MB = 1024 * 1024 Bytes\n",
    "'''\n",
    "print(msg)\n",
    "\n",
    "args = ['MCD15A3H','h08v06','2020','*', '01']\n",
    "hdf_urls = modis_dataset(*args,verbose=True)\n",
    "# test if exist\n",
    "for u in hdf_urls:\n",
    "    print(f'{u.name} : {u.stat().st_size/(1024*1024): .2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility function `modis_dataset` we have developed here is available as `get_url` in the `Modis` class in  `geog0111.modis`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_url in module geog0111.modis:\n",
      "\n",
      "get_url(self, **kwargs)\n",
      "    Get URL object list for NASA MODIS products\n",
      "    for the specified product, tile, year, month, day\n",
      "    \n",
      "    Keyword Arguments:\n",
      "    \n",
      "    verbose:  bool\n",
      "    site    : str \n",
      "    product : str e.g. 'MCD15A3H'\n",
      "    tile    : str e.g. 'h08v06'\n",
      "    year    : str valid 2000-present\n",
      "    month   : str 01-12\n",
      "    day     : str 01-(28,29,30,31)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from  geog0111.modis import Modis\n",
    "\n",
    "help(Modis.get_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCD15A3H.A2020001.h08v06.006.2020006032951.hdf : True\n",
      "MCD15A3H.A2020061.h08v06.006.2020066032716.hdf : False\n",
      "MCD15A3H.A2020153.h08v06.006.2020160231732.hdf : False\n",
      "MCD15A3H.A2020245.h08v06.006.2020253152835.hdf : False\n"
     ]
    }
   ],
   "source": [
    "from  geog0111.modis import Modis\n",
    "\n",
    "modis = Modis(product='MCD15A3H',verbose=False)\n",
    "hdf_urls = modis.get_url(year=\"2020\",month=\"*\",day=\"01\")\n",
    "for u in hdf_urls:\n",
    "    print(f'{u.name} : {u.exists()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "\n",
    "In this section, we have considered URLs and filenames in some detail, and made use of functions from [`pathlib`](https://docs.python.org/3/library/pathlib.html) and [gurlpath](geog0111/gurlpath.py) to access them. These high-level routines will work for a large number of cases. If you need to go deeper into calls to URLs, you should look at the Python packages: [urllib.parse](https://docs.python.org/3.7/library/urllib.parse.html), [requests](https://requests.readthedocs.io/en/master/) and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "The first batch of `Path` commands we saw had much commonality with the core `unix` commands we had previously come across for moving around the file system and finding file information:\n",
    "\n",
    "\n",
    "|command| unix-equivalent | purpose|\n",
    "|---|---|---|\n",
    "|`Path.cwd()`| `pwd` |Return path object representing the current working directory|\n",
    "|`Path.home()`| `~`| Return path object representing the home directory|\n",
    "|`Path.stat()`| `ls -l`* | return info about the path. File size is `Path.stat().st_size` |\n",
    "|`Path.chmod()`| `chmod` | change file mode and permissions|\n",
    "|`Path.mkdir()`| `mkdir` | to create a new directory at the given path|\n",
    "|`Path.rename()`| `mv` | Rename a file or directory to the given target|\n",
    "|`Path.rmdir()`| `rmdir` | Remove the empty directory|\n",
    "|`Path.unlink()`| `rm` | Remove the file or symbolic link|\n",
    "\n",
    "The second set is common to both `Path` and `URL`, and involves queries on the `Path` (`URL`) name, testing for file existence, and using wildcards to access a list of files matching a pattern:\n",
    "\n",
    "|function| purpose|\n",
    "|---|---|\n",
    "|`Path.name`|  filename |\n",
    "|`Path.parent`|  parent |\n",
    "|`Path.parts`|  parts |\n",
    "|`Path.stat()`| return info about the path|\n",
    "|`Path.glob(pattern)`| Glob the pattern given in the URL directory, yielding matching files of any kind| \n",
    "|`Path.exists()`|  test to see if a url is accessible |\n",
    "\n",
    "|function| purpose|\n",
    "|---|---|\n",
    "|`URL.name`|  filename |\n",
    "|`URL.parent`|  parent |\n",
    "|`URL.parts`|  parts |\n",
    "|`URL.stat()`| return info about the file. N.B. Only `URL.stat().st_size` is used for remote files|\n",
    "|`URL.glob(pattern)`| Glob the pattern given in the URL directory, yielding matching files of any kind| \n",
    "|`URL.exists()`|  test to see if a url is accessible |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[<img src=\"images/noun_post_2109127.svg\" width=\"50\" align='right'>](021_Streams.ipynb)[<img src=\"images/noun_pre_2109128.svg\" width=\"50\" align='right'>](018_Running_Python.ipynb)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geog0111-geog0111]",
   "language": "python",
   "name": "conda-env-geog0111-geog0111-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "207px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
