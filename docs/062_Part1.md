# Assessed Practical Part A

## Introduction

### Task overview

In this task, you will be writing codes that download and interpret (convert units) for two datasets of the Del Norte catchment in Colorado, USA for the years 2015-2019 inclusive. The main coding exercise is a Python script that when you run it, downloads the data and stores them in CSV format files. In a notebook, you run the script and show the results. In a second notebook, you produce plots of the datasets. You save the code and two notebooks to PDF files, and submit the work.

### Submission

The due dates for the two formally assessed pieces of coursework are:

        Part A: 16th Nov, 2020 (50% of final mark) - first Monday after reading week.
        Part B: 11th Jan, 2021 (50% of final mark) - first day of term 2

The practical comes in two parts : (A) [data preparation (50%)](062_Part1.md); (B) [modelling (50%)](066_Part2.md) 


Submission is through the usual Turnitin link on the [course Moodle page](https://moodle-1819.ucl.ac.uk/course/view.php?id=2796#section-4). You must submit your notebooks and codes as `pdf` files.


### Checklist
    
* You should be submitting 3 PDF files:
    * work/get_DelNorte_data.py
    * notebook showing running of running work/get_DelNorte_data.py
    * notebook showing graphs of Del Norte data
* None of these files should be more than a few pages long: we do not want you to print or submit the datasets you download.
* Look over the [requirements](#Structure-of-the-Report) to make sure you have completed all parts.
* Make sure you read the advice on [computer codes and plagiarism](A62_Part1_code.md)

##  Background

The hydrology of the Rio Grande Headwaters in Colorado, USA is snowmelt dominated. It varies considerably from year to year and may very further under a changing climate. One of the tools we use to understand monitor processes in such an area is a mathemetical ('environmental') model describing the main physical processes affecting hydrology in the catchment. Such a model could help understand current behaviour and allow some prediction about possible future scenarios. 

In part 2 of your assessment you will be using, calibrating and validating such a model that relates temperature and snow cover in the catchment to river flow. 
![](https://www.blm.gov/sites/blm.gov/files/hero_backgrounds/NM_Rio_Grande_del_Norte_Sign_640.jpg)

We will use the model to describe the streamflow at the Del Norte measurement station, just on the edge of the catchment. You will use environmental (temperature) data and snow cover observations to drive the model. You will perform calibration and testing by comparing model output with observed streamflow data.

### Del Norte

Further general information is available from various [websites](http://www.usclimatedata.com/climate.php?location=USCO0103), including [NOAA](http://www.ncdc.noaa.gov).

![www.coloradofishing.net](http://www.coloradofishing.net/images/fishtails/ft_riogrande3.jpg)



You can visualise the site Del Norte 2E  [here](http://mesonet.agron.iastate.edu/sites/site.php?station=CO2184&network=COCLIMATE).




## Requirements for this submission

In this part of the assessment, you will need to write computer codes to read, interpret and plot the various environmental datasets we will be using in Part 2,


To complete this practical you will need to access, read, save and plot the following datasets:

* **Stream discharge** from the USGS [http://waterdata.usgs.gov](http://waterservices.usgs.gov/nwis/dv/?sites=08220000&format=rdb&startDT=2001-01-01&parameterCd=00060).

* **Temperature**: Temperature data from:
    * [Berkley Earth](http://berkeleyearth.lbl.gov/station-list/stations/32442) OR
    * the Colorado State [Climate data site](http://climate.colostate.edu/data_access.html) OR
    * a [local copy of the data](https://raw.githubusercontent.com/UCL-EO/geog0111/master/data/delNorteT.dat) from the Colorado State Climate data site.
    
The reason for giving multiple access methods for the temperature data is that we have founbf the [Berkley Earth](http://berkeleyearth.lbl.gov/station-list/stations/32442) site to be sometime unresponsive. The problem with the Colorado State [Climate data site](http://climate.colostate.edu/data_access.html) is that you cannot directly query or download the data. Instead you will need to manually save the data, most easily by copying and pasting the data columns into a file. The final option is a copy of the data from the Colorado State [Climate data site](http://climate.colostate.edu/data_access.html). We try to keep this up to date on an annual basis, but cannot guarantee that. You may choose which method you use, but you may find it useful to explore the climate data sites further rather than simply relying on the dataset we have prepared.

## Access

You should examine the data on the site links above and make sure you understand the file format and data characteristics. 

#### Units 

You need to make a note of the units the physical parameters (temperature, discharge etc.) datasets are presented in as you will need to convert them to the units we specify below. The temperature data from the Colorado State [Climate data site](http://climate.colostate.edu/data_access.html) comes in Fahrenheit.

#### Header

Make a note of the number of 'header' lines in the files (if any) and the column headings (if given) as you may need to specify these when reading the dataset.

#### Data access

Before going further, you should use a Jupyter notebook or iPython in a terminal to make sure you can access and interpret these files. There are only two files, so this should not take too long. 

You will most likely want to use `pandas` to do this, and should have gained experience in this already in the course. Try first to use the dataset URL to load into `pandas`, except if you download the Colorado State [Climate data site](http://climate.colostate.edu/data_access.html) to a file (then, read from that file). You may need to try out several options in reading the data into `pandas` if it does not automatically load correctly. In that case, remember that you can skip header lines, and get `pandas` to read only the data lines. You should be able to print the pandas dataframe to see that the data in your table corresponds to what you can see when you access data with a URL.



## Task 1: Data preparation code

In this task, you must write a Python script that you should call `work/get_DelNorte_data.py`. 

* Within this file, create a function called `get_DelNorte_data` that has the following argument:

        year     : integer, between 2000 and the year previous to now (i.e. not this year)
    
  and that returns a dictionary OR `pandas` dataframe containing:

        doy      : array of 365/366 values with the day of year
        flow     : stream flow data for each in units of megalitres/day 
                   (ML/day i.e. units of 1000000 litres a day)
        maxt     : Maximum daily temperature in Celcius (C)

  You can use multiple sub-functions that you call from you function, should you so wish.
  
* Within this file `work/get_DelNorte_data.py` create a function called `write_DelNorte_data` that takes as argument:

        year     : integer, between 2000 and the year previous to now (i.e. not this year)

  then makes a call to `get_DelNorte_data` for the year specified, and saves the dataset in CSV format to a file `work/DelNorte_data_YYYY.csv` each year (replace `YYYY` by the year)

* Within this file `work/get_DelNorte_data.py` create a function called `main` that runs `get_DelNorte_data` for the years 2015 to 2019 inclusive. 

* Construct the Python script so that when you run `work/get_DelNorte_data.py` from a command line, it executes the `main()` function.

* Make the file `work/get_DelNorte_data.py` executable

* In a Python notebook, run the script `work/get_DelNorte_data.py`, and show the size of the files `work/DelNorte_data_*.csv` created.

Hints: Don't forget to include the various packages you need at the top of the file. Don't forget to put docstrings and comments in your file and functions.

## Task 2: Data visualisation

* In a Python notebook, produce plots for the Del Norte site of:

    1. Stream flow data for each year 2015 to 2019 inclusive (in units of megalitres/day - ML/day i.e. units of 1000000 litres a day), as a function of day of year.
    2. Maximum daily temperature in Celcius (C) for each year 2015 to 2019 inclusive as a function of day of year.

You should position the 4 years of data in a 2x2 grid of sub-plots. You code should be well-commented to explain what you are doing. The graphs should be clear, with appropriate titles and axis labels.

### Coursework

You need to submit you coursework in the usual manner by the date given above.

You **must** work individually on this task. If you do not, it will be treated as plagiarism. By reading these instructions for this exercise, we assume that you are aware of the UCL rules on plagiarism. You can find more information on this matter in your student handbook. If in doubt about what might constitute plagiarism, ask one of the course conveners.



### Structure of the Report

The required elements of the report are:

1. Code function `get_DelNorte_data` for temperature and river discharge data download, reading and putting into a dictionary `[40%]`. 
2. Code function `write_DelNorte_data` saves the dataset for a given year to a file in CSV format `[10%]`.
3. Code function `main` that runs get_DelNorte_data for the years 2015 to 2019 inclusive `[10%]`.
4. Additional features of `work/get_DelNorte_data.py` for running as a script `[5%]`.
5. Notebook 1: Demonstration of running the script `work/get_DelNorte_data.py` and the outputs `[15%]`.
6. Notebook 2: Stream flow data plots `[10%]`.
7. Notebook 2: Maximum daily temperature data plots `[10%]`.
    
The figures in brackets indicate the percentage of marks associated with each part. Even if, for some reason, you were unable to generate outputs from your code, you should still be able to pick up marks by attempting all or most of the sections. Most of the marks are given for your computer code, so make sure this is clearly laid out and well-documented. 

Please read [these notes](A03_Part1_code.md) on what we expect of your computer code. 

#### Word limit

There is no word limit *per se* on the computer codes, though as with all writing, you should try to be succinct rather than overly verbose. None of these files should be more than a few pages long: we do not want you to print or submit the datasets you download. You will be marked down if your submission is not clear.

#### Code style

A good to excellent piece of code would take into account issues raised in the [style guide](http://www.python.org/dev/peps/pep-0008/). The ‘degree of excellence’ would depend on how well you take those points on board.
